# ðŸ§® Calculator LLM

A tiny transformer model (~105K parameters) that solves English math problems, built from scratch.

[![Train and Deploy](https://github.com/slahiri/small_calculator_model/actions/workflows/train-and-deploy.yml/badge.svg)](https://github.com/slahiri/small_calculator_model/actions/workflows/train-and-deploy.yml)
[![Hugging Face Space](https://img.shields.io/badge/ðŸ¤—-Live%20Demo-yellow)](https://huggingface.co/spaces/slahiri/small_calculator_model)

## Live Demo

Try it out: [huggingface.co/spaces/slahiri/small_calculator_model](https://huggingface.co/spaces/slahiri/small_calculator_model)

## Quick Start

```bash
# Clone the repo
git clone https://github.com/slahiri/small_calculator_model
cd small_calculator_model

# Install dependencies
pip install -r requirements.txt

# Train the model
cd src
python train.py --output ../output

# Test inference
python generate.py ../output "two plus three"
# Output: two plus three = five
```

## Project Structure

```
small_calculator_model/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ train-and-deploy.yml    # CI/CD: train on push, deploy to HF
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py                # Transformer architecture
â”‚   â”œâ”€â”€ tokenizer.py            # Text â†” token ID conversion
â”‚   â”œâ”€â”€ data.py                 # Training data generation
â”‚   â”œâ”€â”€ train.py                # Training script
â”‚   â””â”€â”€ generate.py             # Inference utilities
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.json             # Model hyperparameters
â”‚   â””â”€â”€ vocab.json              # 36-token vocabulary
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                  # Gradio demo for HF Space
â”‚   â”œâ”€â”€ requirements.txt        # HF Space dependencies
â”‚   â””â”€â”€ README.md               # HF Space metadata
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ full_calculator_llm.ipynb  # Tutorial notebook
â””â”€â”€ requirements.txt            # Training dependencies
```

## Model Architecture

| Property | Value |
|----------|-------|
| Type | Decoder-only Transformer |
| Parameters | ~105K |
| Layers | 2 transformer blocks |
| Embedding Dim | 64 |
| Attention Heads | 4 |
| FF Dim | 256 |
| Vocabulary | 36 tokens |
| Max Sequence | 16 tokens |

## Training

The model trains on ~97K examples covering:
- **Addition**: `a + b` where `a + b â‰¤ 99`
- **Subtraction**: `a - b` where `a - b â‰¥ 0`
- **Multiplication**: `a Ã— b` where `a Ã— b â‰¤ 99`

Test accuracy: **~99%** on held-out test set (no overlap with training).

## CI/CD Pipeline

On push to `main`:
1. **Train**: Run training on GitHub Actions (CPU, ~50 mins)
2. **Validate**: Ensure test accuracy â‰¥ 95%
3. **Deploy**: Push model to Hugging Face Space

### Setup

Add `HF_TOKEN` to your repository secrets:
1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Create a token with write access
3. Add to GitHub: Settings â†’ Secrets â†’ Actions â†’ `HF_TOKEN`

## Tutorial

This model was built following: [sid.sh/learn/build-your-first-llm](https://sid.sh/learn/build-your-first-llm)

## License

MIT
